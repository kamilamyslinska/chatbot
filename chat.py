# -*- coding: utf-8 -*-
"""jagiello.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xSzOz_IsYpurUkc1D4YZQ8oKO6WwYaf6
"""

!pip install newspaper3k

"""# Skraping Article

"""

from newspaper import Article
import random
import string
import pandas as pd
import numpy as np
import warnings
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

#ignore the warnings
warnings.filterwarnings('ignore')

#download package from nltk
nltk.download('punkt',quiet=True)
nltk.download('wordnet',quiet=True)

#get artical url
article= Article('http://www.poczet.com/jagiello.htm')
article.download()
article.parse()
article.nlp()
corpus=article.text
#print
print(corpus)

text=corpus
sent_tokens=nltk.sent_tokenize(text)
print(sent_tokens)

remove_punct_dict=dict( (ord(punct),None) for punct in string.punctuation)
print(string.punctuation)
print(remove_punct_dict)

def LemNormalize(text):
  return nltk.word_tokenize(text.lower().translate(remove_punct_dict))
print(LemNormalize(text))

greeting_input=["hi","hello","hey","hola", "cześć"]
greeting_response=["howdy","hey there","hi","hello :)", "cześć"]
def greeting(sentence):
  for word in sentence.split():
    if word.lower() in greeting_input:
      return random.choice(greeting_response)

def response(user_response): 
  user_response=user_response.lower()
  #print(user_response)
  #robo response
  robo_response=''
  sent_tokens.append(user_response)
  #print(sent_tokens)
  tfidfvec=TfidfVectorizer(tokenizer=LemNormalize , stop_words='english')
  tfidf=tfidfvec.fit_transform(sent_tokens)
  #print(tfidf)
    val=cosine_similarity(tfidf[-1],tfidf)
  #print(val)
  idx=val.argsort()[0][-2]
  flat=val.flatten()
  flat.sort()
  score=flat[-2]
  #print(score)
  if score==0:
    robo_response=robo_response+"sorry,nie rozumiem"
  else:
    robo_response=robo_response+sent_tokens[idx]

  sent_tokens.remove(user_response)
  return robo_response

greeting_input=["hey","cześć"]
greeting_response=["hey","hi","hello :)","cześć"]
def greeting(sentence):
  for word in sentence.split():
    if word.lower() in greeting_input:
      return random.choice(greeting_response)
flag=True
print("hello!!! Jestem Jagiełło, Mogę odpowiedzieć na pytania o mnie, wystarczy wpisać hasło i nacisnąć enter, napisz koniec żeby wyjść")
while(flag==True):
  user_response=input("TY :):")
  #user_response=user_response.lower()
  if(user_response!='koniec'):
    if(user_response=='dziękuję' or user_response=='dziękuję bardzo'):
      flag=False
      print("Jagiełło: nie ma problemu :)")
    else:
       if( greeting(user_response) != None):
         print("Jagiełło: "+ greeting(user_response))
       else:
         print("Jagiełło:"+response(user_response))
  else:
    flag=False
    print("Jagiełło: do zobaczenia :)")

"""#Wikipedia"""

!pip install wikipedia

!python3 -m spacy download pl_core_news_sm

import wikipedia
import requests
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import spacy
nlp = spacy.load("pl_core_news_sm")

print(spacy.__version__)

wikipedia.set_lang("pl")

result = wikipedia.search("Władysław II Jagiełło")
result

page = wikipedia.page(result[0], preload= True)

doc = nlp(page.content)

print(doc)

df = pd.DataFrame({'caly':[doc]})

replacer ={'\n':'', '\\n':'',"[\[].*?[\]]": " ",'[!"#%\'()*+,-./:;<=>?@\[\]^_`{|}’”“′‘\\\]':" "}
df['cleanText'] = df['caly'].replace(replacer, regex=True)
df.head()

print(df.dtypes)

text = df['cleanText']
text

def LemNormalize(doc):
    tekst1 = nlp(doc)
    return [token.text for token in tekst1]
print(LemNormalize(doc))

from spacy.tokenizer import Tokenizer

tekst = [sent.text for sent in doc.sents]

doc1 = nlp('tron, koronacja')
doc2 = nlp(tekst[5])
sim = doc1.similarity(doc2)

doc1 = nlp('tron, koronacja')
for zdanie in tekst:
  doc2 = nlp(zdanie)
  print(doc1.similarity(doc2))

data = doc
train_corpus = nlp(data)
train_corpus = nlp(" ".join([sent.text for sent in doc.sents]))
test_corpus = nlp('król')
ae = train_corpus.similarity(test_corpus)

#słowa kluczowe dla powitań
greeting_input=["cześć","hello","hey"]
greeting_response=["cześć","hey","hello :)"]
def greeting(sentence):
  for word in sentence.split():
    if word.lower() in greeting_input:
      return random.choice(greeting_response)

from spacy.util import SimpleFrozenDict
def response(user_response):
  user_response=user_response.lower()
  #print(user_response)
  #robo response
  doc1 = nlp(user_response)
  robo_response=''
  for zdanie in tekst:
    doc2 = nlp(zdanie)
    sim = doc1.similarity(doc2)
  sent_tokens = tekst
  val = np.array(sim)
  idx= val.argmax()
  score= idx
  #print(score)
  if score==0:
    robo_response=robo_response+"sorry,nie rozumiem"
  else:
    robo_response=robo_response+sent_tokens

  sent_tokens.remove(user_response)
  return robo_response

greeting_input=["cześć","hello","hey"]
greeting_response=["cześć","hey","hello :)"]
def greeting(sentence):
  for word in sentence.split():
    if word.lower() in greeting_input:
      return random.choice(greeting_response)
flag=True
print("Cześć jestem Jagiełło, mogę Ci o sobie opowiedzieć, jeśli chcesz wyjść napisz koniec")
while(flag==True):
  user_response=input("TY:")
  #user_response=user_response.lower()
  if(user_response!='koniec'):
    if(user_response=='Dzięki' or user_response=='Dziękuję'):
      flag=False
      print("Jagiełło: Nie ma problemu :)")
    else:
       if( greeting(user_response) != None):
         print("Jagiełło: "+ greeting(user_response))
       else:
         print("Jagiełło: "+response(user_response))
  else:
    flag=False
    print("Jagiełło: Do zobaczenia :)")